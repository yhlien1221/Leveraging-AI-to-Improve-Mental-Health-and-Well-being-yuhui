# -*- coding: utf-8 -*-
"""„ÄåTask 3_llama2-chatbot.ipynb„ÄçÁöÑÂâØÊú¨

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sBgGYlJJngQ-p8G3BCl0jnFLBMjGB3Lt
"""

# app.py
import streamlit as st
from ctransformers import AutoModelForCausalLM

# 1) Load model using ctransformers
@st.cache_resource
def load_model():
    return AutoModelForCausalLM.from_pretrained(
        "TheBloke/Llama-2-7B-Chat-GGML",
        model_file="llama-2-7b-chat.ggmlv3.q4_0.bin",  # You must download this model
        model_type="llama",
        gpu_layers=32  # Adjust for your GPU
    )

llm = load_model()

# 2) Streamlit UI
st.title("üß† Mental Health Counselor Chatbot")
st.caption("üí¨ Powered by ctransformers LLaMA-2")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Display previous chat
for role, msg in st.session_state.chat_history:
    with st.chat_message(role):
        st.markdown(msg)

# New input
if user_input := st.chat_input("You:"):
    st.chat_message("user").markdown(user_input)
    st.session_state.chat_history.append(("user", user_input))

    # Rebuild prompt
    MAX_HISTORY_TURNS = 4
    hist = st.session_state.chat_history[-MAX_HISTORY_TURNS*2:]
    prompt = ""
    for role, msg in hist:
        prompt += f"{'User' if role=='user' else 'Bot'}: {msg}\n"
    prompt += "Bot:"

    # Generate reply
    reply = llm(prompt, max_new_tokens=150)
    reply = reply.split("User:")[0].strip()

    st.chat_message("assistant").markdown(reply)
    st.session_state.chat_history.append(("assistant", reply))