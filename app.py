# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UwMLc8LBewNPo_VuA07t3q9xVzvz7zz9
"""

import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load fine-tuned model and tokenizer
model_path = "model"        # Folder where your model is saved
tokenizer_path = "tokenizer"  # Folder where your tokenizer is saved

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

# Title
st.title("ðŸ§  Mental Health Chatbot (Fine-tuned DialoGPT)")

# Text input
user_input = st.text_input("You:", "")

if user_input:
    # Create prompt
    input_text = f"User: {user_input} <|sep|> Bot:"

    # Tokenize input
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    # Generate output
    output_ids = model.generate(
        input_ids,
        max_length=150,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        temperature=0.7,
        top_k=50,
        top_p=0.95
    )

    # Decode and display response
    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    bot_reply = response.split("<|sep|> Bot:")[-1].strip()

    st.markdown(f"**Bot:** {bot_reply}")